{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation script to annotate collected rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import pathlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "import openai\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'experiment_log.json' with the path to your actual JSON file\n",
    "ROOT_PATH = pathlib.Path(\"__file__\").resolve().parent.parent\n",
    "EXP_FOLDER = os.path.join(ROOT_PATH, \"experiments\")\n",
    "# Define the experiment ID\n",
    "experiment_id = 'ft_23_from_bt_7_pair_personal' # Replace with your actual experiment ID\n",
    "logs_id = 'bt_7_p1_train_pair' # last iteration of collected logs from which the new model learn from\n",
    "OUTPUT_PATH = os.path.join(EXP_FOLDER, experiment_id)\n",
    "LOGS_FOLDER = os.path.join(ROOT_PATH, \"logs\")\n",
    "CONFIGS_FOLDER = os.path.join(ROOT_PATH, \"cos_eor\", \"configs\", \"local\")\n",
    "ENVS_FILE_PATH = os.path.join(CONFIGS_FOLDER , \"envs_demo.yaml\")\n",
    "\n",
    "\n",
    "# Note: put the OpenAI key here:\n",
    "with open(os.path.join(CONFIGS_FOLDER, \"api_key.yaml\")) as kfile:\n",
    "    k = yaml.safe_load(kfile)\n",
    "openai.api_key = k['key'] # PUT THE API_KEY into key.txt file\n",
    "if 'organization' in k:\n",
    "    openai.organization = k['organization']\n",
    "FINETUNE_MODEL = \"TO BE ADDED\" # the base model from the last iter\n",
    "SUFFIX = f\"TO BE ADDED\" # suffix to be used for the new output fine-tune model, e.g., ft_23_from_bt\n",
    "\n",
    "TRAIN_FILE_NAME_OUTPUT = \"train.jsonl\"\n",
    "VALID_FILE_NAME_OUTPUT = \"valid.jsonl\"\n",
    "TEST_FILE_NAME_OUTPUT = \"test.jsonl\"\n",
    "META_FILE_NAME = \"info.txt\"\n",
    "\n",
    "# Constants\n",
    "ANNOTATION = \"annotation\"\n",
    "DIFF_CORRECT_LOC = \"diff_correct_loc\"\n",
    "EXPERIMENT = \"experiment\"\n",
    "FLAG = \"flag\"\n",
    "FINETUNE_MSG = \"finetune_message\"\n",
    "NUM_OBJECTS_DISCOVERED = \"num_objects_discovered\"\n",
    "NUM_RECS_DISCOVERED = \"num_recs_discovered\"\n",
    "OUTCOME = \"outcome\"\n",
    "PROMPT = \"prompt\"\n",
    "REWARD = \"reward\"\n",
    "REWARD_WEIGHTS = {NUM_OBJECTS_DISCOVERED: 1, NUM_RECS_DISCOVERED: 1, DIFF_CORRECT_LOC: 10}\n",
    "SCENE = \"scene\"\n",
    "SUC = \"succeeded\"\n",
    "SUC_STEPS = \"successful_steps\"\n",
    "\n",
    "# Read the scene IDs from the envs.yaml file\n",
    "with open(ENVS_FILE_PATH , 'r') as file:\n",
    "    scenes = yaml.safe_load(file).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod val 35\n"
     ]
    }
   ],
   "source": [
    "# Load the scene IDs from the envs.yaml file\n",
    "def load_scenes(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Function to get log file paths for a given experiment ID and scene ID\n",
    "def get_experiment_log_paths(logs_id, scene_id, mode='train'):\n",
    "    logs_path_pattern = f'{LOGS_FOLDER}/{logs_id}/demo/{scene_id}/data_*.json'\n",
    "    all_paths = sorted(glob.glob(logs_path_pattern), reverse=False) # sorted ascending according to time\n",
    "    # we need to get the train episodes for train, test episodes for evaluation\n",
    "    log_paths = []\n",
    "    if mode == 'all':\n",
    "        return all_paths\n",
    "    if mode == 'train':\n",
    "        if 'ablationlarge' in logs_id:\n",
    "            MOD_VAL = 35\n",
    "        elif 'ablationsmall' in logs_id:\n",
    "            MOD_VAL = 10\n",
    "        elif 'small' in logs_id:\n",
    "            MOD_VAL = 10\n",
    "        elif \"large\" in logs_id:\n",
    "            MOD_VAL = 25\n",
    "        elif \"pair\" in logs_id:\n",
    "            MOD_VAL = 25\n",
    "        else:\n",
    "            MOD_VAL = 10\n",
    "            print ('no experiment size provided, assume small experiment')\n",
    "        print ('mod val', MOD_VAL)\n",
    "        for i, log in enumerate(all_paths):\n",
    "            if i % MOD_VAL >= 5:\n",
    "                # log 5 - 9 are for training\n",
    "                log_paths.append(log)\n",
    "            else:\n",
    "                # log 0 - 4 are for testing\n",
    "                continue\n",
    "    return log_paths\n",
    "\n",
    "def reward(result):\n",
    "    reward = sum(result[k] * REWARD_WEIGHTS[k] for k in REWARD_WEIGHTS)\n",
    "    return reward\n",
    "\n",
    "def compile_steps(steps):\n",
    "    # Enumerate over the steps, starting at 1, and format them into a string\n",
    "    nl = \"\\n\".join(f\"step {index}: {step}\" for index, step in enumerate(steps, start=1))\n",
    "    nl_without_prefix = nl.replace('step 1: ', '')\n",
    "    return nl_without_prefix\n",
    "\n",
    "def prepare_example_conversation(system_msg, user_msg, assistant_message):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": system_msg,})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def annotate_record(record, logs_id, scene_id):\n",
    "    result = {}\n",
    "    result[SCENE] = scene_id\n",
    "    result[EXPERIMENT] = logs_id\n",
    "    result[NUM_OBJECTS_DISCOVERED] = len(record[OUTCOME][\"objects_discovered\"])\n",
    "    result[NUM_RECS_DISCOVERED] = len(record[OUTCOME][\"recs_discovered\"])\n",
    "    result[DIFF_CORRECT_LOC] = record[OUTCOME][\"count_correct\"][\"end\"] - record[OUTCOME][\"count_correct\"][\"start\"]\n",
    "    # craft a response based on successful steps\n",
    "    result[SUC_STEPS] = [l['step_raw'] for l in record[\"logs\"] if l[FLAG] == SUC]\n",
    "    system_msg = record[\"low_level\"][\"prompt\"][\"system\"]\n",
    "    user_msg = record[\"low_level\"][\"prompt\"][\"user\"]\n",
    "    assistant_msg = compile_steps(result[SUC_STEPS])\n",
    "    result[FINETUNE_MSG] = prepare_example_conversation(system_msg=system_msg, user_msg=user_msg, assistant_message=assistant_msg)\n",
    "    result[REWARD] = reward(result)\n",
    "    return result\n",
    "\n",
    "# Function to load experiment logs and add the scene name\n",
    "def load_and_annotate_logs(logs_id, scenes):\n",
    "    all_records = []\n",
    "    all_episodes = []\n",
    "    for scene_id in scenes:\n",
    "        log_file_paths = get_experiment_log_paths(logs_id, scene_id, mode='train')\n",
    "        for log_file_path in log_file_paths:\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                records = json.load(file)\n",
    "                # Annotate each record with the scene name\n",
    "                for record in records:\n",
    "                    record[ANNOTATION] = annotate_record(record, logs_id, scene_id)\n",
    "                all_records.extend(records)\n",
    "                all_episodes.append(records)\n",
    "    return all_records, all_episodes\n",
    "\n",
    "# Load and annotate logs\n",
    "annotated_logs, annotated_episodes = load_and_annotate_logs(logs_id, scenes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pomaria_1_int': 327})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_records = [log for log in annotated_logs if log['annotation']['diff_correct_loc'] > 0]\n",
    "pick_records = [log for log in annotated_logs if log['annotation']]\n",
    "\n",
    "from collections import Counter\n",
    "Counter([log[ANNOTATION][SCENE] for log in positive_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_episode_steps = []\n",
    "for ie, episode in enumerate(annotated_episodes):\n",
    "    flag_correct = False\n",
    "    for ir, record in enumerate(episode):\n",
    "        if record[ANNOTATION]['diff_correct_loc'] > 0:\n",
    "            correct_episode_steps.append((ie, ir)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct step episodes 327\n",
      "Avg correct steps per episode 2.18\n",
      "[(0, 1), (0, 3), (0, 6), (1, 2), (1, 4), (2, 4), (2, 5), (3, 2), (3, 3), (4, 1), (4, 2), (4, 4), (4, 5), (5, 3), (6, 1), (6, 9), (7, 2), (7, 3), (7, 4), (8, 2), (8, 3), (8, 6), (8, 8), (9, 1), (9, 2), (10, 1), (10, 2), (10, 3), (11, 1), (13, 1), (14, 1), (14, 5), (14, 7), (15, 1), (16, 1), (16, 2), (16, 5), (16, 7), (17, 1), (17, 4), (18, 2), (19, 1), (20, 1), (20, 2), (20, 6), (20, 8), (20, 9), (20, 10), (20, 11), (21, 1), (21, 6), (22, 1), (22, 2), (22, 5), (23, 3), (23, 5), (24, 1), (24, 5), (25, 2), (25, 3), (26, 3), (26, 5), (27, 1), (27, 2), (27, 3), (28, 1), (28, 2), (28, 5), (29, 1), (29, 5), (29, 6), (30, 4), (30, 5), (30, 9), (30, 10), (31, 1), (32, 3), (32, 6), (32, 7), (33, 2), (33, 4), (34, 1), (34, 3), (34, 4), (34, 5), (35, 7), (36, 1), (36, 4), (36, 6), (37, 1), (37, 3), (37, 5), (38, 3), (38, 4), (38, 6), (39, 1), (39, 4), (40, 2), (40, 5), (40, 8), (42, 2), (43, 5), (44, 1), (44, 3), (45, 1), (46, 1), (46, 8), (47, 1), (48, 4), (49, 1), (50, 3), (50, 4), (50, 6), (51, 2), (51, 5), (52, 1), (52, 2), (52, 3), (53, 2), (53, 7), (54, 1), (54, 5), (54, 7), (55, 2), (55, 6), (55, 8), (56, 2), (57, 2), (57, 5), (58, 1), (58, 2), (58, 8), (59, 1), (60, 1), (60, 2), (60, 4), (60, 12), (61, 2), (61, 3), (62, 2), (62, 5), (63, 1), (63, 3), (64, 1), (64, 2), (64, 5), (65, 5), (66, 1), (66, 7), (67, 1), (67, 8), (67, 9), (68, 1), (68, 4), (68, 5), (68, 9), (69, 1), (70, 1), (70, 5), (70, 6), (70, 10), (71, 1), (71, 6), (73, 1), (73, 2), (73, 5), (74, 1), (75, 1), (76, 1), (76, 5), (77, 1), (77, 3), (78, 2), (79, 1), (79, 2), (80, 1), (80, 2), (80, 3), (80, 11), (81, 2), (82, 1), (82, 2), (82, 3), (83, 2), (83, 7), (84, 1), (85, 4), (86, 1), (87, 1), (87, 4), (87, 5), (88, 1), (88, 2), (89, 1), (90, 1), (90, 3), (90, 5), (90, 9), (92, 5), (93, 2), (93, 4), (94, 1), (94, 3), (94, 7), (95, 3), (95, 8), (96, 1), (96, 6), (97, 4), (97, 6), (98, 2), (98, 3), (99, 1), (99, 2), (100, 1), (100, 2), (100, 6), (101, 1), (102, 1), (102, 2), (103, 1), (104, 1), (104, 5), (105, 2), (106, 1), (107, 2), (107, 3), (107, 7), (108, 2), (108, 3), (109, 5), (110, 1), (110, 6), (110, 8), (111, 4), (112, 1), (112, 2), (112, 3), (112, 4), (113, 2), (113, 4), (114, 1), (114, 2), (115, 1), (115, 3), (115, 7), (116, 4), (117, 1), (117, 2), (117, 5), (118, 1), (118, 2), (118, 6), (119, 2), (119, 5), (120, 1), (120, 2), (121, 1), (121, 2), (122, 1), (122, 3), (122, 7), (123, 1), (123, 2), (124, 1), (124, 2), (124, 3), (125, 3), (125, 7), (126, 2), (126, 5), (126, 8), (127, 1), (127, 2), (127, 6), (128, 3), (128, 4), (128, 5), (128, 7), (129, 1), (129, 3), (130, 1), (130, 3), (131, 3), (132, 1), (132, 5), (133, 1), (133, 3), (133, 6), (134, 1), (135, 1), (136, 4), (136, 5), (136, 6), (137, 1), (137, 2), (137, 6), (138, 7), (139, 1), (139, 6), (140, 1), (140, 4), (140, 6), (140, 11), (141, 2), (142, 1), (142, 2), (142, 3), (142, 4), (143, 4), (144, 2), (144, 7), (144, 10), (145, 1), (145, 2), (145, 5), (146, 2), (146, 4), (146, 5), (147, 2), (147, 5), (147, 7), (148, 1), (148, 2), (149, 1), (149, 4), (149, 5)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print ('Number of correct step episodes', len(correct_episode_steps))\n",
    "print ('Avg correct steps per episode', len(correct_episode_steps)/len(annotated_episodes))\n",
    "print (correct_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pomaria_1_int'\n",
      "['go to candy 1',\n",
      " 'look at candy 1',\n",
      " 'pick up candy 1',\n",
      " 'go to pantry room 0 top cabinet 55',\n",
      " 'look at pantry room 0 top cabinet 55',\n",
      " 'place candy 1 on pantry room 0 top cabinet 55']\n",
      "{'end': {'candy 1': 'pantry room 0 top cabinet 55',\n",
      "         'cloth 1': 'pantry room 0 top cabinet 54',\n",
      "         'dumbbell rack 1': 'pantry room 0 top cabinet 54',\n",
      "         'umbrella 1': 'pantry room 0 top cabinet 55'},\n",
      " 'start': {'cloth 1': 'pantry room 0 top cabinet 54',\n",
      "           'dumbbell rack 1': 'pantry room 0 top cabinet 54',\n",
      "           'umbrella 1': 'pantry room 0 top cabinet 55'}}\n"
     ]
    }
   ],
   "source": [
    "EPISODE_INDEX = 0\n",
    "STEP_INDEX = 1\n",
    "pprint (annotated_episodes[EPISODE_INDEX][STEP_INDEX][ANNOTATION][SCENE])\n",
    "pprint (annotated_episodes[EPISODE_INDEX][STEP_INDEX][ANNOTATION]['successful_steps'])\n",
    "pprint (annotated_episodes[EPISODE_INDEX][STEP_INDEX]['correct_objects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune records 327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'You are a one-handed household robot.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'There are objects misplaced on wrong receptacles and potentially in the wrong room. \\nYou are holding nothing.\\nIn corridor 0, found receptacles: corridor 0 carpet 13, corridor 0 shelf 14, corridor 0 table 0. \\nIn kitchen 0, found receptacles: kitchen 0 counter 40, kitchen 0 dishwasher 36, kitchen 0 bottom cabinet 35, kitchen 0 top cabinet 42, kitchen 0 sink 37, kitchen 0 counter 49, kitchen 0 bottom cabinet 44, kitchen 0 counter 48, kitchen 0 carpet 34, kitchen 0 cooktop 64, kitchen 0 chair 12, kitchen 0 oven 46, kitchen 0 plant 5, kitchen 0 chair 7, kitchen 0 table 6, kitchen 0 bottom cabinet 47, kitchen 0 counter 33, kitchen 0 bottom cabinet 32, kitchen 0 counter 31, kitchen 0 oven 21, kitchen 0 oven 22, kitchen 0 bottom cabinet 26, kitchen 0 bottom cabinet 29, kitchen 0 bottom cabinet 30, kitchen 0 top cabinet 27, kitchen 0 fridge 20, kitchen 0 microwave 23, kitchen 0 top cabinet 24, kitchen 0 top cabinet 25, kitchen 0 chair 8, kitchen 0 chair 9, kitchen 0 chair 10, kitchen 0 chair 11, kitchen 0 top cabinet 43, kitchen 0 bottom cabinet 39. Object candy 1 found on kitchen 0 bottom cabinet 26. Object skillet 1 found on kitchen 0 table 6 \\nIn living room 0, found receptacles: living room 0 chair 1, living room 0 shelf 16, living room 0 coffee table 3, living room 0 coffee table 4, living room 0 chair 2, living room 0 shelf 15. \\nIn pantry room 0, found receptacles: pantry room 0 counter 53, pantry room 0 bottom cabinet 51, pantry room 0 bottom cabinet 52, pantry room 0 fridge 50, pantry room 0 top cabinet 54, pantry room 0 top cabinet 55. Object dumbbell rack 1 found on pantry room 0 top cabinet 54. Object hair straightener 1 found on pantry room 0 counter 53. Object racquetball 1 found on pantry room 0 fridge 50 \\nIn utility room 0, found receptacles: utility room 0 dryer 57, utility room 0 washer 56, utility room 0 sink 58, utility room 0 bottom cabinet 59, utility room 0 top cabinet 60. \\nIn bathroom 0, found receptacles: bathroom 0 mirror 63, bathroom 0 sink 62.. \\nGive me the next steps to explore the house and place misplaced objects on correct receptacles. Use the following actions for each step and separate by new lines: go to room, go to object, go to receptacle, look at object, look at receptacle, pick up object, place object on receptacle. . \\nExample steps to explore room living room 0:\\nstep 1: go to living room 0\\nExample steps to pick up saucer 1 and place on kitchen 0 counter 18:\\nstep 1: go to saucer 1\\nstep 2: look at saurcer 1\\nstep 3: pick up saucer 1\\nstep 4: go to kitchen 0 counter 18\\nstep 5: look at kitchen 0 counter 18\\nstep 6: place saurcer 1 on kitchen 0 counter 18\\n. The next steps should only follow one of the above examples. If complete, print mission complete as the next step. Steps: '},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'go to candy 1\\nstep 2: look at candy 1\\nstep 3: pick up candy 1\\nstep 4: go to pantry room 0 top cabinet 55\\nstep 5: look at pantry room 0 top cabinet 55\\nstep 6: place candy 1 on pantry room 0 top cabinet 55'}]}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_records = [log for log in annotated_logs if log[ANNOTATION][DIFF_CORRECT_LOC] > 0]\n",
    "print ('finetune records', len(finetune_records))\n",
    "finetune_records[0][ANNOTATION][FINETUNE_MSG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove duplicates\n",
    "(keep the first of groupby same 'object_moved', 'rec_before', 'rec_after', 'correct_before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(annotated_logs)\n",
    "df_all['object_moved'] = df_all.apply(lambda x: list(x['outcome']['objects_moved'].keys())[0] if len(x['outcome']['objects_moved'].keys()) > 0 else None, axis=1)\n",
    "df_all['rec_before'] = df_all.apply(lambda x: list(x['outcome']['objects_moved'].values())[0][0] if len(x['outcome']['objects_moved'].keys()) > 0 else None, axis=1)\n",
    "df_all['rec_after'] = df_all.apply(lambda x: list(x['outcome']['objects_moved'].values())[0][1] if len(x['outcome']['objects_moved'].keys()) > 0 else None, axis=1)\n",
    "df_all['correct_before'] = df_all.apply(lambda x: str(set(x['correct_objects']['start'])), axis=1)\n",
    "df_all_remove_duplicate = df_all.groupby(['object_moved', 'rec_before', 'rec_after', 'correct_before']).first().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all finetune records before duplicate removal 327\n",
      "finetune records after duplicate removal 259\n",
      "num correct objects 48\n"
     ]
    }
   ],
   "source": [
    "# let's see if we could de-duplicate before merging\n",
    "df_finetune = pd.DataFrame(finetune_records)\n",
    "df_finetune['object_moved'] = df_finetune.apply(lambda x: list(x['outcome']['objects_moved'].keys())[0], axis=1)\n",
    "df_finetune['rec_before'] = df_finetune.apply(lambda x: list(x['outcome']['objects_moved'].values())[0][0], axis=1)\n",
    "df_finetune['rec_after'] = df_finetune.apply(lambda x: list(x['outcome']['objects_moved'].values())[0][1], axis=1)\n",
    "df_finetune['correct_before'] = df_finetune.apply(lambda x: str(set(x['correct_objects']['start'])), axis=1)\n",
    "df_remove_duplicate = df_finetune.groupby(['object_moved', 'rec_before', 'rec_after', 'correct_before']).first().reset_index()\n",
    "finetune_msg_simulation = df_remove_duplicate.apply(lambda x: x[ANNOTATION][FINETUNE_MSG], axis=1).to_list()\n",
    "print ('all finetune records before duplicate removal', len(df_finetune))\n",
    "print ('finetune records after duplicate removal', len(finetune_msg_simulation))\n",
    "print ('num correct objects', len(df_finetune.groupby(['object_moved']).first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "move forward to write messages to jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/disk/scratch1/dhan/workplace/housekeep-dev/experiments/annotate_gpt3_pair.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bam1/disk/scratch1/dhan/workplace/housekeep-dev/experiments/annotate_gpt3_pair.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmove forward to write messages to jsonl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: move forward to write messages to jsonl"
     ]
    }
   ],
   "source": [
    "raise Exception('move forward to write messages to jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all positive records 259\n"
     ]
    }
   ],
   "source": [
    "finetune_records = finetune_msg_simulation # [log for log in annotated_logs if log[ANNOTATION][DIFF_CORRECT_LOC] > 0]\n",
    "NUM_VALID = 0\n",
    "NUM_TRAIN = len(finetune_records) - NUM_VALID\n",
    "print ('all positive records', len(finetune_records))\n",
    "training_data = finetune_records[:NUM_TRAIN]# [log[ANNOTATION][FINETUNE_MSG] for log in finetune_records[:400]]\n",
    "validation_data = finetune_records[NUM_TRAIN:] # [log[ANNOTATION][FINETUNE_MSG] for log in finetune_records[400:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)\n",
    "            \n",
    "def write_metadata_file(filepath):\n",
    "    with open(filepath, 'w') as out:\n",
    "        flag = \"\"\n",
    "        flag += f\"Number of training samples: {len(training_data)}\\n\"\n",
    "        flag += f\"Number of validation samples: {len(validation_data)}\\n\"\n",
    "        flag += f\"Source folder: {logs_id}\"\n",
    "        out.write(flag)\n",
    "        print (flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 259\n",
      "Number of validation samples: 0\n",
      "Source folder: ft_36_from_bt_14_ablationlarge\n"
     ]
    }
   ],
   "source": [
    "training_file_name = os.path.join(OUTPUT_PATH, TRAIN_FILE_NAME_OUTPUT)\n",
    "# Create the parent directory if it doesn't exist\n",
    "Path(training_file_name).parent.mkdir(parents=True, exist_ok=True)\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "if NUM_VALID > 0:\n",
    "    validation_file_name = os.path.join(OUTPUT_PATH, VALID_FILE_NAME_OUTPUT)\n",
    "    write_jsonl(validation_data, validation_file_name)\n",
    "\n",
    "write_metadata_file(os.path.join(OUTPUT_PATH, META_FILE_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "move forward to create finetune jobs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/disk/scratch1/dhan/workplace/housekeep-dev/experiments/annotate_gpt3_pair.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bam1/disk/scratch1/dhan/workplace/housekeep-dev/experiments/annotate_gpt3_pair.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmove forward to create finetune jobs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: move forward to create finetune jobs"
     ]
    }
   ],
   "source": [
    "raise Exception('move forward to create finetune jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetune_upload_response():\n",
    "    training_response = openai.File.create(file=open(training_file_name, \"rb\"), purpose=\"fine-tune\")\n",
    "    training_file_id = training_response[\"id\"]\n",
    "    print(\"Training file ID:\", training_file_id)\n",
    "    if NUM_VALID > 0:\n",
    "        validation_response = openai.File.create(file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\")\n",
    "        validation_file_id = validation_response[\"id\"]\n",
    "        print(\"Validation file ID:\", validation_file_id)\n",
    "        return {\"training_response\": training_response, \"validation_response\": validation_response}\n",
    "    else:\n",
    "        return {\"training_response\": training_response}\n",
    "\n",
    "def create_finetune_response_and_log(training_file_id, validation_file_id=None):\n",
    "    if validation_file_id is not None:\n",
    "        response = openai.FineTuningJob.create( training_file=training_file_id, validation_file=validation_file_id, model=FINETUNE_MODEL, suffix=SUFFIX, \\\n",
    "            hyperparameters={\"n_epochs\":1})\n",
    "    else:\n",
    "        response = openai.FineTuningJob.create( training_file=training_file_id, model=FINETUNE_MODEL, suffix=SUFFIX, \\\n",
    "            hyperparameters={\"n_epochs\":1})\n",
    "    job_id = response[\"id\"]\n",
    "    with open(os.path.join(OUTPUT_PATH, \"job_info.txt\"), 'w') as out:\n",
    "        flag = \"\"\n",
    "        flag += f\"training file id: {training_file_id}\\n\"\n",
    "        flag += f\"validation file id: {validation_file_id}\\n\"\n",
    "        flag += f\"finetune job id: {job_id}\\n\"\n",
    "        flag += f\"finetune model: {FINETUNE_MODEL}\\n\"\n",
    "        flag += f\"finetune suffix: {SUFFIX}\"\n",
    "        out.write(flag)\n",
    "        print(\"Status:\", response[\"status\"])\n",
    "        print(\"Job ID:\", response[\"id\"])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-iPycS4D30kHVZX5nYTtV0eh9\n",
      "Status: validating_files\n",
      "Job ID: ftjob-RwpQtzgv7GZt2qIlw7FO0tIy\n"
     ]
    }
   ],
   "source": [
    "upload_result = create_finetune_upload_response()\n",
    "training_response = upload_result['training_response']\n",
    "validation_file_id = None\n",
    "if NUM_VALID > 0:\n",
    "    validation_response = upload_result['validation_response']\n",
    "    validation_file_id = validation_response['id']\n",
    "training_file_id = training_response['id']\n",
    "\n",
    "finetune_response = create_finetune_response_and_log(training_file_id, validation_file_id)\n",
    "job_id = finetune_response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-RwpQtzgv7GZt2qIlw7FO0tIy\n",
      "Status: validating_files\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "\n",
    "print(\"Job ID:\", response[\"id\"])\n",
    "print(\"Status:\", response[\"status\"])\n",
    "print(\"Trained Tokens:\", response[\"trained_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaelic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fde01e40ce835f40519108abea24a46644765d2b0dbe10643cc34bafc8aa5e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
