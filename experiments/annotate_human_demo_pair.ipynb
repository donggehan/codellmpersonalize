{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation script to annotate collected rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pathlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "import openai\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE PARAMETERS START --------------\n",
    "# Define the experiment ID\n",
    "logs_id = 'single_run_21_demo_easy'\n",
    "experiment_id = 'bt_7_p1_train_pair' # Replace with your actual experiment ID\n",
    "FINETUNE_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "SUFFIX = f\"bt_7_p1\"\n",
    "FLAG_FILTER = True # if true, filter out episodes with failed steps, if false, keep all demonstrations as is\n",
    "STARTING_EPISODE = 25\n",
    "NUM_EPISODES_PER_SCENE = 12\n",
    "# CONFIGURE PARAMETERS END --------------\n",
    "\n",
    "# Replace 'experiment_log.json' with the path to your actual JSON file\n",
    "ROOT_PATH = pathlib.Path(\"__file__\").resolve().parent.parent\n",
    "EXP_FOLDER = os.path.join(ROOT_PATH, \"experiments\")\n",
    "\n",
    "OUTPUT_PATH = os.path.join(EXP_FOLDER, experiment_id)\n",
    "LOGS_FOLDER = os.path.join(ROOT_PATH, \"logs\")\n",
    "CONFIGS_FOLDER = os.path.join(ROOT_PATH, \"cos_eor\", \"configs\", \"local\")\n",
    "ENVS_FILE_PATH = os.path.join(CONFIGS_FOLDER , \"envs_demo.yaml\")\n",
    "\n",
    "# Note: put the OpenAI key here:\n",
    "with open(os.path.join(CONFIGS_FOLDER, \"api_key.yaml\")) as kfile:\n",
    "    k = yaml.safe_load(kfile)\n",
    "openai.api_key = k['key'] # PUT THE API_KEY into key.txt file\n",
    "if 'organization' in k:\n",
    "    openai.organization = k['organization']\n",
    "\n",
    "TRAIN_FILE_NAME_OUTPUT = \"train.jsonl\"\n",
    "VALID_FILE_NAME_OUTPUT = \"valid.jsonl\"\n",
    "TEST_FILE_NAME_OUTPUT = \"test.jsonl\"\n",
    "META_FILE_NAME = \"info.yaml\"\n",
    "\n",
    "# Constants\n",
    "ANNOTATION = \"annotation\"\n",
    "EPISODE = \"episode\"\n",
    "DIFF_CORRECT_LOC = \"diff_correct_loc\"\n",
    "EXPERIMENT = \"experiment\"\n",
    "FLAG = \"flag\"\n",
    "FINETUNE_MSG = \"finetune_message\"\n",
    "NUM_OBJECTS_DISCOVERED = \"num_objects_discovered\"\n",
    "NUM_RECS_DISCOVERED = \"num_recs_discovered\"\n",
    "MISSION_COMPLETE = \"mission complete\"\n",
    "OUTCOME = \"outcome\"\n",
    "PROMPT = \"prompt\"\n",
    "REWARD = \"reward\"\n",
    "REWARD_WEIGHTS = {NUM_OBJECTS_DISCOVERED: 1, NUM_RECS_DISCOVERED: 1, DIFF_CORRECT_LOC: 10}\n",
    "SCENE = \"scene\"\n",
    "SUC = \"succeeded\"\n",
    "FAIL = \"failed\"\n",
    "SKIP = \"skipped\"\n",
    "SUC_STEPS = \"successful_steps\"\n",
    "\n",
    "# Read the scene IDs from the envs.yaml file\n",
    "with open(ENVS_FILE_PATH , 'r') as file:\n",
    "    scenes = yaml.safe_load(file).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_paths ['/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-44-55.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-45-22.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-46-02.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-46-35.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-47-00.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-47-33.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-48-13.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-48-37.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-49-04.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-49-20.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-49-41.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-50-16.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-50-38.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-51-18.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-51-38.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-51-52.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-52-11.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-52-30.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-53-11.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-53-36.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-54-00.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-54-38.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-55-03.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-55-17.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-55-46.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-56-08.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-56-42.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-57-13.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-57-38.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-58-03.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-58-37.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-58-59.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-59-23.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-59-37.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_22-59-53.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-00-23.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-00-45.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-01-07.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-01-39.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-02-07.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-02-23.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-02-43.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-03-07.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-03-34.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-04-08.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-04-32.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-04-58.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-05-38.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-05-59.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-06-19.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-06-45.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-07-12.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-07-34.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-08-12.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-08-30.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-08-57.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-09-23.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-09-42.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-10-23.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-10-44.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-11-08.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-11-31.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-11-51.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-12-15.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-12-38.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-13-11.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-13-43.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-14-08.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-14-41.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-15-04.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-15-28.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-15-54.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-16-14.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-16-55.json', '/disk/scratch1/dhan/workplace/housekeep-dev/logs/single_run_21_demo_easy/demo/pomaria_1_int/data_2023-11-23_23-17-16.json']\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "failed log\n",
      "successful episodes {'pomaria_1_int': [26, 29, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41]}\n"
     ]
    }
   ],
   "source": [
    "# Load the scene IDs from the envs.yaml file\n",
    "def load_scenes(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Function to get log file paths for a given experiment ID and scene ID\n",
    "def get_experiment_log_paths(experiment_id, scene_id):\n",
    "    logs_path_pattern = f'{LOGS_FOLDER}/{experiment_id}/demo/{scene_id}/data_*.json'\n",
    "    all_paths = sorted(glob.glob(logs_path_pattern), reverse=False)[STARTING_EPISODE:]\n",
    "    print ('all_paths', all_paths)\n",
    "    return all_paths\n",
    "\n",
    "def reward(result):\n",
    "    reward = sum(result[k] * REWARD_WEIGHTS[k] for k in REWARD_WEIGHTS)\n",
    "    return reward\n",
    "\n",
    "def compile_steps(steps):\n",
    "    # Enumerate over the steps, starting at 1, and format them into a string\n",
    "    nl = \"\\n\".join(f\"step {index}: {step}\" for index, step in enumerate(steps, start=1))\n",
    "    nl_without_prefix = nl.replace('step 1: ', '')\n",
    "    return nl_without_prefix\n",
    "\n",
    "def prepare_example_conversation(system_msg, user_msg, assistant_message):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": system_msg,})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def annotate_record(record, experiment_id, scene_id, episode_id):\n",
    "    result = {}\n",
    "    result[SCENE] = scene_id\n",
    "    result[EXPERIMENT] = experiment_id\n",
    "    result[EPISODE] = episode_id\n",
    "    result[NUM_OBJECTS_DISCOVERED] = len(record[OUTCOME][\"objects_discovered\"])\n",
    "    result[NUM_RECS_DISCOVERED] = len(record[OUTCOME][\"recs_discovered\"])\n",
    "    result[DIFF_CORRECT_LOC] = record[OUTCOME][\"count_correct\"][\"end\"] - record[OUTCOME][\"count_correct\"][\"start\"]\n",
    "    # craft a response based on successful steps\n",
    "    result[\"all_steps\"] = [l['step_raw'] for l in record['logs']]\n",
    "    result[SUC_STEPS] = [l['step_raw'] for l in record[\"logs\"] if l[FLAG] == SUC]\n",
    "    system_msg = record[\"low_level\"][\"prompt\"][\"system\"]\n",
    "    user_msg = record[\"low_level\"][\"prompt\"][\"user\"] + \"\\nstep 1: \" \n",
    "    assistant_msg = compile_steps(result[\"all_steps\"]) # result[SUC_STEPS])\n",
    "    result[FINETUNE_MSG] = prepare_example_conversation(system_msg=system_msg, user_msg=user_msg, assistant_message=assistant_msg)\n",
    "    result[REWARD] = reward(result)\n",
    "    return result\n",
    "\n",
    "def annotate_episode(episode, experiment_id, scene_id):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Function to load experiment logs and add the scene name\n",
    "def load_and_annotate_logs(experiment_id, scenes):\n",
    "    all_records = []\n",
    "    all_episodes = []\n",
    "    suc_episode_ids = {}\n",
    "    for scene_id in scenes:\n",
    "        suc_episode_ids[scene_id] = []\n",
    "        log_file_paths = get_experiment_log_paths(experiment_id, scene_id)\n",
    "        for i, log_file_path in enumerate(log_file_paths):\n",
    "            if len(suc_episode_ids[scene_id]) >= NUM_EPISODES_PER_SCENE:\n",
    "                break\n",
    "            episode_id = i + STARTING_EPISODE\n",
    "            with open(log_file_path, 'r') as file:\n",
    "                records = json.load(file)\n",
    "                # Annotate each record with the scene name\n",
    "                for record in records:\n",
    "                    record[ANNOTATION] = annotate_record(record, experiment_id, scene_id, episode_id)\n",
    "                all_records.extend(records)\n",
    "                # append an episode only if it doesn't contain failed steps\n",
    "                filter_out_episode = False\n",
    "                for record in records:\n",
    "                    failed_logs = [1 for l in record['logs'] if l[FLAG] == FAIL]\n",
    "                    skipped_logs = [l for l in record['logs'] if l[FLAG] == SKIP and l['step_raw'] != MISSION_COMPLETE]\n",
    "                    if FLAG_FILTER and len(failed_logs + skipped_logs) > 0:\n",
    "                        filter_out_episode = True\n",
    "                        print ('failed log')\n",
    "                        break\n",
    "                if not filter_out_episode:\n",
    "                    suc_episode_ids[scene_id].append(episode_id)\n",
    "                    all_episodes.append(records)\n",
    "                # annotated_episode = annotate_episode(records, experiment_id, scene_id)\n",
    "    return all_records, all_episodes, suc_episode_ids\n",
    "\n",
    "# Load and annotate logs\n",
    "annotated_logs, annotated_episodes, suc_episode_ids = load_and_annotate_logs(logs_id, scenes)\n",
    "print ('successful episodes', suc_episode_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pomaria_1_int': 40})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_records = [log for log in annotated_logs if log['annotation']['diff_correct_loc'] > 0]\n",
    "pick_records = [log for log in annotated_logs if log['annotation']]\n",
    "\n",
    "from collections import Counter\n",
    "Counter([log[ANNOTATION][SCENE] for log in positive_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_episode_steps = []\n",
    "for ie, episode in enumerate(annotated_episodes):\n",
    "    flag_correct = False\n",
    "    for ir, record in enumerate(episode):\n",
    "        if record[ANNOTATION]['diff_correct_loc'] > 0:\n",
    "            correct_episode_steps.append((ie, ir)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct step episodes 32\n",
      "[(0, 1), (0, 2), (0, 3), (0, 4), (1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3), (4, 1), (4, 2), (4, 3), (5, 1), (6, 1), (6, 2), (7, 1), (7, 2), (7, 3), (7, 4), (8, 1), (8, 2), (8, 3), (9, 1), (9, 2), (9, 3), (10, 1), (11, 1), (11, 2)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print ('Number of correct step episodes', len(correct_episode_steps))\n",
    "print (correct_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats to log: the objects across seeds within the same scene\n",
    "# success rates of the generated plans\n",
    "# receptacle counts\n",
    "from collections import defaultdict\n",
    "object_stats = defaultdict(list) # scene, seed -> object lists\n",
    "msgs_counts = defaultdict(list)\n",
    "gt_correct_counts = defaultdict(list) # successful if all wrong objects cleared\n",
    "mission_complete_counts = defaultdict(list) # episode ends with mission complete message\n",
    "\n",
    "correct_counts = defaultdict(list)\n",
    "for ie, episode in enumerate(annotated_episodes):\n",
    "    scene = episode[0][ANNOTATION][SCENE]\n",
    "    objects = list(episode[0]['logs'][0]['current_mapping']['start'].keys())\n",
    "    msgs_counts[scene].append(len(episode))\n",
    "    object_stats[scene].append(objects)\n",
    "    diff = episode[-1][OUTCOME][\"count_correct\"]['end'] - episode[0][OUTCOME]['count_correct']['start']\n",
    "    gt_diff = episode[-1][OUTCOME]['count_correct']['end'] + episode[-1][OUTCOME]['count_wrong']['end'] - episode[0][OUTCOME]['count_correct']['start']\n",
    "    gt_correct_counts[scene].append(gt_diff)\n",
    "    correct_counts[scene].append(diff)\n",
    "    last_msg_mission_complete = episode[-1]['logs'][-1]['step_raw'] == MISSION_COMPLETE\n",
    "    if last_msg_mission_complete:\n",
    "        mission_complete_counts[scene].append(1)\n",
    "    else:\n",
    "        mission_complete_counts[scene].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pomaria_1_int\n",
      "59 unique objects 8.416666666666666 objects per episode on average of  12 episodes\n",
      "all unique object count 59\n"
     ]
    }
   ],
   "source": [
    "# object counts\n",
    "all_unique_objects = set()\n",
    "for scene in object_stats:\n",
    "    print (scene)\n",
    "    unique_objects = set()\n",
    "    all_objects_count = []\n",
    "    for objects in object_stats[scene]:\n",
    "        unique_objects = unique_objects.union(objects)\n",
    "        all_unique_objects = all_unique_objects.union(objects)\n",
    "        all_objects_count.append(len(objects))\n",
    "    print (len(unique_objects), 'unique objects', sum(all_objects_count)/len(all_objects_count), 'objects per episode on average of ', len(all_objects_count), 'episodes')\n",
    "print ('all unique object count', len(all_unique_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pomaria_1_int\n",
      "average 4.666666666666667 messages per episode\n",
      "In total 56 messages.\n"
     ]
    }
   ],
   "source": [
    "# number of messages counts\n",
    "total_msgs = 0\n",
    "for scene in msgs_counts:\n",
    "    print (scene)\n",
    "    total_msgs += sum(msgs_counts[scene])\n",
    "    print ('average', sum(msgs_counts[scene])/len(msgs_counts[scene]), 'messages per episode')\n",
    "print ('In total', total_msgs, 'messages.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pomaria_1_int\n",
      "[Actual] ~ 2.6666666666666665 diff correct objects per episode\n",
      "[Ground Truth] ~ 3.5833333333333335 diff correct objects per episode\n",
      "[Ratio] success rate 74.4186046511628%\n",
      "[mission complete] 12 out of 12 | rate = 100.0%\n"
     ]
    }
   ],
   "source": [
    "# diff rates and success rates (total number of diff)\n",
    "for scene in correct_counts:\n",
    "    print (scene)\n",
    "    print ('[Actual] ~', sum(correct_counts[scene])/len(correct_counts[scene]), 'diff correct objects per episode')\n",
    "    print ('[Ground Truth] ~', sum(gt_correct_counts[scene])/len(gt_correct_counts[scene]), 'diff correct objects per episode')\n",
    "    suc_rate = sum(correct_counts[scene])/sum(gt_correct_counts[scene]) * 100\n",
    "    print ('[Ratio] success rate', f'{suc_rate}%' )\n",
    "    num_mission_complete = sum(mission_complete_counts[scene])\n",
    "    num_all_episodes = len(mission_complete_counts[scene])\n",
    "    mission_complete_rate = num_mission_complete/num_all_episodes * 100\n",
    "    print (f'[{MISSION_COMPLETE}] {num_mission_complete} out of {num_all_episodes} | rate = {mission_complete_rate}%')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "move forward to write messages to jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove forward to write messages to jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: move forward to write messages to jsonl"
     ]
    }
   ],
   "source": [
    "raise Exception('move forward to write messages to jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPISODES = 10\n",
    "NUM_VALID_EPISODES = 2\n",
    "\n",
    "training_data = []\n",
    "validation_data = []\n",
    "episode_counter = {}\n",
    "for episode in annotated_episodes:\n",
    "    scene = episode[0][ANNOTATION][SCENE]\n",
    "    if scene in episode_counter:\n",
    "        episode_counter[scene] += 1\n",
    "    else:\n",
    "        episode_counter[scene] = 1\n",
    "    if episode_counter[scene] > NUM_TRAIN_EPISODES:\n",
    "        for log in episode:\n",
    "            validation_data.append(log[ANNOTATION][FINETUNE_MSG])\n",
    "    else:\n",
    "        for log in episode:\n",
    "            training_data.append(log[ANNOTATION][FINETUNE_MSG])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print (len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)\n",
    "            \n",
    "def write_metadata_file(filepath):\n",
    "    with open(filepath, 'w') as out:\n",
    "        flag = {}\n",
    "        flag['logs_id'] = logs_id\n",
    "        flag['scenes'] = scenes\n",
    "        flag['starting_episode'] = STARTING_EPISODE\n",
    "        flag['num_episodes_per_scene'] = NUM_EPISODES_PER_SCENE\n",
    "        flag['chosen_episodes_per_scene'] = suc_episode_ids\n",
    "        flag['num_train_episodes'] = NUM_TRAIN_EPISODES\n",
    "        flag['num_valid_episodes'] = NUM_VALID_EPISODES\n",
    "        flag['num_train_samples'] = len(training_data)\n",
    "        flag['num_valid_samples'] = len(validation_data) \n",
    "        yaml.dump(flag, out, default_flow_style=False)\n",
    "        print (flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logs_id': 'single_run_21_demo_easy', 'scenes': ['pomaria_1_int'], 'starting_episode': 35, 'num_episodes_per_scene': 22, 'chosen_episodes_per_scene': {'pomaria_1_int': [35, 36, 37, 39, 40, 41, 42, 44, 45, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61]}, 'num_train_episodes': 20, 'num_valid_episodes': 2, 'num_train_samples': 102, 'num_valid_samples': 11}\n"
     ]
    }
   ],
   "source": [
    "training_file_name = os.path.join(OUTPUT_PATH, TRAIN_FILE_NAME_OUTPUT)\n",
    "# Create the parent directory if it doesn't exist\n",
    "Path(training_file_name).parent.mkdir(parents=True, exist_ok=True)\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "if NUM_VALID_EPISODES > 0:\n",
    "    validation_file_name = os.path.join(OUTPUT_PATH, VALID_FILE_NAME_OUTPUT)\n",
    "    write_jsonl(validation_data, validation_file_name)\n",
    "\n",
    "write_metadata_file(os.path.join(OUTPUT_PATH, META_FILE_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "move forward to create finetune jobs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/disk/scratch1/dhan/workplace/housekeep-dev/experiments/annotate_human_demo_pair.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bam1/disk/scratch1/dhan/workplace/housekeep-dev/experiments/annotate_human_demo_pair.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmove forward to create finetune jobs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: move forward to create finetune jobs"
     ]
    }
   ],
   "source": [
    "raise Exception('move forward to create finetune jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetune_upload_response():\n",
    "    training_response = openai.File.create(file=open(training_file_name, \"rb\"), purpose=\"fine-tune\")\n",
    "    training_file_id = training_response[\"id\"]\n",
    "    print(\"Training file ID:\", training_file_id)\n",
    "    if NUM_VALID_EPISODES > 0:\n",
    "        validation_response = openai.File.create(file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\")\n",
    "        validation_file_id = validation_response[\"id\"]\n",
    "        print(\"Validation file ID:\", validation_file_id)\n",
    "        return {\"training_response\": training_response, \"validation_response\": validation_response}\n",
    "    else:\n",
    "        return {\"training_response\": training_response}\n",
    "\n",
    "\n",
    "def create_finetune_response_and_log(training_file_id, validation_file_id=None):\n",
    "    if validation_file_id is not None:\n",
    "        response = openai.FineTuningJob.create( training_file=training_file_id, validation_file=validation_file_id, model=FINETUNE_MODEL, suffix=SUFFIX, hyperparameters={\"n_epochs\":1})\n",
    "    else:\n",
    "        response = openai.FineTuningJob.create( training_file=training_file_id, model=FINETUNE_MODEL, suffix=SUFFIX, hyperparameters={\"n_epochs\":1})\n",
    "    job_id = response[\"id\"]\n",
    "    with open(os.path.join(OUTPUT_PATH, \"job_info.txt\"), 'w') as out:\n",
    "        flag = \"\"\n",
    "        flag += f\"training file id: {training_file_id}\\n\"\n",
    "        if validation_file_id is not None:\n",
    "            flag += f\"validation file id: {validation_file_id}\\n\"\n",
    "        flag += f\"finetune job id: {job_id}\\n\"\n",
    "        flag += f\"finetune model: {FINETUNE_MODEL}\\n\"\n",
    "        flag += f\"finetune suffix: {SUFFIX}\"\n",
    "        out.write(flag)\n",
    "        print(\"Status:\", response[\"status\"])\n",
    "        print(\"Job ID:\", response[\"id\"])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-Wa81ITeapyRnu0YquSft9qAR\n",
      "Validation file ID: file-nmDKnps4BsnIZUoKXFlFget0\n",
      "Status: validating_files\n",
      "Job ID: ftjob-9QFGXPmQTudY238LTj3B1mTx\n"
     ]
    }
   ],
   "source": [
    "upload_result = create_finetune_upload_response()\n",
    "training_response = upload_result['training_response']\n",
    "validation_file_id = None\n",
    "training_file_id = training_response['id']\n",
    "if NUM_VALID_EPISODES > 0:\n",
    "    validation_response = upload_result['validation_response']\n",
    "    validation_file_id = validation_response['id']\n",
    "finetune_response = create_finetune_response_and_log(training_file_id, validation_file_id)\n",
    "job_id = finetune_response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-9QFGXPmQTudY238LTj3B1mTx\n",
      "Status: validating_files\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "\n",
    "print(\"Job ID:\", response[\"id\"])\n",
    "print(\"Status:\", response[\"status\"])\n",
    "print(\"Trained Tokens:\", response[\"trained_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaelic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fde01e40ce835f40519108abea24a46644765d2b0dbe10643cc34bafc8aa5e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
